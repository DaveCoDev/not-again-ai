{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPT-4V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use GPT-4V's image capabilities directly through the OpenAI API. \n",
    "We provide helper functions to simplify the creation of prompts and understanding which parameters are available while maintaining the complete flexibility that the API offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Prompts\n",
    "\n",
    "Prompts for GPT-4V follow the familiar [chat completion](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) format as the non-vision enabled models like `gpt-35-turbo` and `gpt-4-turbo`.\n",
    "\n",
    "A chat completion prompt for one of those models looks as follows:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Vision enabled LLMs require a slightly different format. Images are available to the models in two ways: by passing a URL to an image or by passing the base64 encoded image directly in the request. Note that images can be passed in the `user`, `system` and `assistant` messages, however currently they cannot be in the *first* message.\n",
    "\n",
    "An example of a prompt containing multiple text and image messages is as follows:\n",
    "\n",
    "```python\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "cat_image_path = Path(\"cat.jpg\")\n",
    "dog_image_path = Path(\"dog.jpg\")\n",
    "\n",
    "def encode_image(image_path: Path) -> str:\n",
    "    with Path.open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe the animal in the image in one word.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(cat_image)}\", \"detail\": \"low\"},\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"Cat\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What about this animal?\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(dog_image)}\", \"detail\": \"high\"},\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "Notice that we can have messages containing text as before. When we want to include images with a message, `content` becomes a list. That list can contain both text and image messages, in any order. We used the `encode_image` function to convert the image to base64 encoding. The optional `detail` parameter in the `image_url` message specifies the quality of the image. It can be either `low` or `high`. For more details on how images are processed and associated costs, refer to the [OpenAI API documentation](https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding).\n",
    "\n",
    "Next let's use some helper functions to create another prompt that we will use to send a request to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'Based on these infographics, can you summarize how Semantic Kernel works in exactly one sentence?'},\n",
       "   {'type': 'image_url',\n",
       "    'image_url': {'url': 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAADKsA...',\n",
       "     'detail': 'high'}},\n",
       "   {'type': 'image_url',\n",
       "    'image_url': {'url': 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAADWAA...',\n",
       "     'detail': 'low'}}]}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from not_again_ai.llm.prompts import chat_prompt_vision\n",
    "\n",
    "sk_infographic = Path.cwd().parent.parent / \"tests\" / \"llm\" / \"sample_images\" / \"SKInfographic.png\"\n",
    "sk_diagram = Path.cwd().parent.parent / \"tests\" / \"llm\" / \"sample_images\" / \"SKDiagram.png\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful {{ persona }}.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            \"Based on these infographics, can you summarize how {{ library }} works in exactly one sentence?\",\n",
    "            {\"image\": sk_infographic, \"detail\": \"high\"},\n",
    "            {\"image\": sk_diagram, \"detail\": \"low\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = chat_prompt_vision(messages, variables={\"persona\": \"assistant\", \"library\": \"Semantic Kernel\"})\n",
    "\n",
    "# Truncate the url fields to avoid cluttering the output\n",
    "prompt[1][\"content\"][1][\"image_url\"][\"url\"] = prompt[1][\"content\"][1][\"image_url\"][\"url\"][0:50] + \"...\"\n",
    "prompt[1][\"content\"][2][\"image_url\"][\"url\"] = prompt[1][\"content\"][2][\"image_url\"][\"url\"][0:50] + \"...\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the two images that were encoded:\n",
    "\n",
    "![SKInfographic](https://github.com/DaveCoDev/not-again-ai/blob/main/tests/llm/sample_images/SKInfographic.png?raw=true)\n",
    "\n",
    "![SKDiagram](https://github.com/DaveCoDev/not-again-ai/blob/main/tests/llm/sample_images/SKDiagram.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making an API Request\n",
    "\n",
    "With prompt formatted, making the request is easy. \n",
    "\n",
    "One consideration is that currently the vision models do not support many parameters of the text only models such as: message.name, functions/tools, and response_format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9B3vmPITPDY4ggJGCgn7FAApRD0ur', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Semantic Kernel works by leveraging a core processing unit that selects and invokes AI services based on the application's prompts, manages template application and response parsing, ensures reliability, and handles telemetry and monitoring, facilitating a structured and responsible AI operations framework.\", role='assistant', function_call=None, tool_calls=None))], created=1712423746, model='gpt-4-1106-vision-preview', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=47, prompt_tokens=1565, total_tokens=1612))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = chat_prompt_vision(messages, variables={\"persona\": \"assistant\", \"library\": \"Semantic Kernel\"})\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-1106-vision-preview\",\n",
    "    messages=prompt,\n",
    "    max_tokens=200,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying the response format\n",
    "\n",
    "The response from the API is quite verbose. We can simplify it by extracting only what is needed, depending on the parameters we provided in our request. \n",
    "\n",
    "Using our helper functions, let's send a request which tries to use all the available parameters. Notice that we use `n=2` to get two completions in one request. However, due to the seed they should always be equivalent. NOTE: We have noticed that the `seed` parameter is hit or miss and does not generate the same completions in all scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'finish_reason': 'stop',\n",
       "   'message': 'Semantic Kernel operates by selecting and invoking AI services based on a given context, processing the response through a central kernel that manages reliability and monitoring, and then delivering the processed result back to the application.'},\n",
       "  {'finish_reason': 'stop',\n",
       "   'message': 'Semantic Kernel processes input by selecting an AI service based on the context, templating and rendering the prompt, invoking the selected AI service, and parsing the response to create a functional result that is integrated into an application while handling reliability, telemetry, and monitoring.'}],\n",
       " 'completion_tokens': 90,\n",
       " 'prompt_tokens': 1565}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from not_again_ai.llm.chat_completion_vision import chat_completion_vision\n",
    "from not_again_ai.llm.openai_client import openai_client\n",
    "\n",
    "client = openai_client()\n",
    "\n",
    "response = chat_completion_vision(\n",
    "    messages=prompt, model=\"gpt-4-1106-vision-preview\", client=client, max_tokens=200, temperature=0.5, seed=42, n=2\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finish_reason': 'stop',\n",
       " 'message': 'Semantic Kernel operates by selecting and invoking AI services based on a given context, processing the response through a central kernel that manages reliability and monitoring, and then delivering the processed result back to the application.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we choose the default `n=1`, the response would look more like this:\n",
    "response[\"choices\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
