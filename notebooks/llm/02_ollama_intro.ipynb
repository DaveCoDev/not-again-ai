{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Ollama\n",
    "\n",
    "[Ollama](https://github.com/ollama/ollama) is a simple way to get started with running language models locally.\n",
    "\n",
    "We provide helpers to interface with Ollama by wrapping the [ollama-python](https://github.com/ollama/ollama-python) package.\n",
    "\n",
    "## Installation\n",
    "\n",
    "See the main README for installation instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the Ollama client\n",
    "\n",
    "We use the `Client` class from Ollama to allow customizability of the host. By default, the `ollama_client` function will try to read in the `OLLAMA_HOST` environment variable. If it is not set, you must provide a host. Generally, the default is `http://localhost:11434`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from not_again_ai.llm.chat_completion.providers.ollama_api import ollama_client\n",
    "\n",
    "client = ollama_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chat Completion\n",
    "\n",
    "The same `chat_completion` used for OpenAI, etc can be used to call models hosted on Ollama.\n",
    "\n",
    "We assume that the model `phi4` has already been pulled into Ollama. If not, you can do so with the command `ollama pull phi4` in your terminal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(choices=[ChatCompletionChoice(message=AssistantMessage(content=\"Hi there! How can I assist you today? Whether it's answering questions, providing information, or helping with a specific task, feel free to let me know what you need! ðŸ˜Š\", role=<Role.ASSISTANT: 'assistant'>, name=None, refusal=None, tool_calls=None), finish_reason='stop', json_message=None, logprobs=None, extras=None)], errors='', completion_tokens=39, prompt_tokens=24, completion_detailed_tokens=None, prompt_detailed_tokens=None, response_duration=4.5843, system_fingerprint=None, extras=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from not_again_ai.llm.chat_completion import chat_completion\n",
    "from not_again_ai.llm.chat_completion.types import ChatCompletionRequest, SystemMessage, UserMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    UserMessage(content=\"Hello!\"),\n",
    "]\n",
    "\n",
    "request = ChatCompletionRequest(\n",
    "    messages=messages,\n",
    "    model=\"phi4\",\n",
    "    context_window=4000,  # Set context_window because Ollama's default is small.\n",
    ")\n",
    "\n",
    "response = chat_completion(request, provider=\"ollama\", client=client)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion with Other Features\n",
    "\n",
    "The Ollama API also supports several other features, such as JSON mode, temperature, and max_tokens. The `ChatCompletionRequest` class has fields for all of these including ones specific to Ollama such as `top_k`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(choices=[ChatCompletionChoice(message=AssistantMessage(content='{\\n    \"random_number\": 47\\n} \\n\\n', role=<Role.ASSISTANT: 'assistant'>, name=None, refusal=None, tool_calls=None), finish_reason='stop', json_message={'random_number': 47}, logprobs=None, extras=None)], errors='', completion_tokens=12, prompt_tokens=40, completion_detailed_tokens=None, prompt_detailed_tokens=None, response_duration=4.258, system_fingerprint=None, extras=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    UserMessage(content=\"Generate a random number between 0 and 100 and structure the response in using JSON.\"),\n",
    "]\n",
    "\n",
    "request = ChatCompletionRequest(\n",
    "    messages=messages,\n",
    "    model=\"phi4\",\n",
    "    max_completion_tokens=300,\n",
    "    context_window=1000,\n",
    "    temperature=1.51,\n",
    "    json_mode=True,\n",
    "    top_k=5,\n",
    "    seed=6,\n",
    ")\n",
    "\n",
    "response = chat_completion(request, provider=\"ollama\", client=client)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
