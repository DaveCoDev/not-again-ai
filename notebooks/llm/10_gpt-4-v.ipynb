{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPT-4V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use GPT-4V's image capabilities directly through the OpenAI API.\n",
    "We provide helper functions to simplify the creation of prompts and understanding which parameters are available while maintaining the complete flexibility that the API offers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Prompts\n",
    "\n",
    "Prompts for vision enabled models follow the familiar [chat completion](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) format as the non-vision enabled models or requests.\n",
    "\n",
    "However, including images in the prompt requires a slightly different format. Images are available to the models in two ways: by passing a URL to an image or by passing the base64 encoded image directly in the request.\n",
    "Note that images can be passed in the `user`, `system` and `assistant` messages, however currently they cannot be in the _first_ message [[source]](https://platform.openai.com/docs/guides/vision).\n",
    "\n",
    "We can have messages containing text as before, but when we want to include images with a message, `content` becomes a list. That list can contain both text and image messages, in any order. We used the `encode_image` function to convert the image to base64 encoding. The optional `detail` parameter in the `image_url` message specifies the quality of the image. It can be either `low` or `high`. For more details on how images are processed and associated costs, refer to the [OpenAI API documentation](https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding). Other providers may not have this functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', role=<Role.SYSTEM: 'system'>, name=None),\n",
       " UserMessage(content=[TextContent(type=<ContentPartType.TEXT: 'text'>, text='Based on these infographics, can you summarize how Semantic Kernel works in exactly one sentence?'), ImageContent(type=<ContentPartType.IMAGE: 'image_url'>, image_url=ImageUrl(url='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAADKsA...', detail=<ImageDetail.HIGH: 'high'>)), ImageContent(type=<ContentPartType.IMAGE: 'image_url'>, image_url=ImageUrl(url='data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAADWAA...', detail=<ImageDetail.LOW: 'low'>))], role=<Role.USER: 'user'>, name=None)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from not_again_ai.llm.chat_completion.types import (\n",
    "    ImageContent,\n",
    "    ImageDetail,\n",
    "    ImageUrl,\n",
    "    SystemMessage,\n",
    "    TextContent,\n",
    "    UserMessage,\n",
    ")\n",
    "from not_again_ai.llm.prompting.compile_prompt import compile_messages, encode_image\n",
    "\n",
    "sk_infographic = Path.cwd().parent.parent / \"tests\" / \"llm\" / \"sample_images\" / \"SKInfographic.png\"\n",
    "sk_diagram = Path.cwd().parent.parent / \"tests\" / \"llm\" / \"sample_images\" / \"SKDiagram.png\"\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful {{ persona }}.\"),\n",
    "    UserMessage(\n",
    "        content=[\n",
    "            TextContent(\n",
    "                text=\"Based on these infographics, can you summarize how {{ library }} works in exactly one sentence?\"\n",
    "            ),\n",
    "            ImageContent(\n",
    "                image_url=ImageUrl(url=f\"data:image/png;base64,{encode_image(sk_infographic)}\", detail=ImageDetail.HIGH)\n",
    "            ),\n",
    "            ImageContent(\n",
    "                image_url=ImageUrl(url=f\"data:image/png;base64,{encode_image(sk_diagram)}\", detail=ImageDetail.LOW)\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = compile_messages(messages, variables={\"persona\": \"assistant\", \"library\": \"Semantic Kernel\"})\n",
    "\n",
    "# Truncate the url fields to avoid cluttering the output\n",
    "prompt[1].content[1].image_url.url = prompt[1].content[1].image_url.url[0:50] + \"...\"\n",
    "prompt[1].content[2].image_url.url = prompt[1].content[2].image_url.url[0:50] + \"...\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the two images that were encoded:\n",
    "\n",
    "![SKInfographic](https://github.com/DaveCoDev/not-again-ai/blob/main/tests/llm/sample_images/SKInfographic.png?raw=true)\n",
    "\n",
    "![SKDiagram](https://github.com/DaveCoDev/not-again-ai/blob/main/tests/llm/sample_images/SKDiagram.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making an API Request\n",
    "\n",
    "With prompt formatted, making the request is easy.\n",
    "\n",
    "### Simplifying the response format\n",
    "\n",
    "The response from the API is quite verbose. We can simplify it by extracting only what is needed, depending on the parameters we provided in our request.\n",
    "\n",
    "Using our helper functions, let's send a request which tries to use all the available parameters. Notice that we use `n=2` to get two completions in one request. However, due to the seed they should always be equivalent. NOTE: We have noticed that the `seed` parameter is hit or miss and does not generate the same completions in all scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Semantic Kernel is a framework that integrates various AI services and plugins to manage and execute tasks by processing prompts, utilizing memory, planning, and invoking functions to deliver results efficiently.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from not_again_ai.llm.chat_completion import chat_completion\n",
    "from not_again_ai.llm.chat_completion.providers.openai_api import openai_client\n",
    "from not_again_ai.llm.chat_completion.types import ChatCompletionRequest\n",
    "\n",
    "client = openai_client()\n",
    "\n",
    "prompt = compile_messages(messages, variables={\"persona\": \"assistant\", \"library\": \"Semantic Kernel\"})\n",
    "\n",
    "request = ChatCompletionRequest(\n",
    "    messages=prompt,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    max_completion_tokens=200,\n",
    "    temperature=0.5,\n",
    "    seed=42,\n",
    "    n=2,\n",
    ")\n",
    "response = chat_completion(request, \"openai\", client)\n",
    "response.choices[0].message.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
