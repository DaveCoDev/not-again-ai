{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPT-4V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use GPT-4V's image capabilities directly through the OpenAI API. \n",
    "We provide helper functions to simplify the creation of prompts and understanding which parameters are available while maintaining the complete flexibility that the API offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Prompts\n",
    "\n",
    "Prompts for GPT-4V follow the familiar [chat completion](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) format as the non-vision enabled models like `gpt-35-turbo`, `gpt-4-turbo`, and `gpt-4o`.\n",
    "\n",
    "A chat completion prompt for one of those models looks as follows:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Vision enabled LLMs require a slightly different format. Images are available to the models in two ways: by passing a URL to an image or by passing the base64 encoded image directly in the request. \n",
    "Note that images can be passed in the `user`, `system` and `assistant` messages, however currently they cannot be in the *first* message [[source]](https://platform.openai.com/docs/guides/vision).\n",
    "\n",
    "An example of a prompt containing multiple text and image messages is as follows:\n",
    "\n",
    "```python\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "cat_image_path = Path(\"cat.jpg\")\n",
    "dog_image_path = Path(\"dog.jpg\")\n",
    "\n",
    "def encode_image(image_path: Path) -> str:\n",
    "    with Path.open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe the animal in the image in one word.\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(cat_image)}\", \"detail\": \"low\"},\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"Cat\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What about this animal?\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(dog_image)}\", \"detail\": \"high\"},\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "Notice that we can have messages containing text as before. When we want to include images with a message, `content` becomes a list. That list can contain both text and image messages, in any order. We used the `encode_image` function to convert the image to base64 encoding. The optional `detail` parameter in the `image_url` message specifies the quality of the image. It can be either `low` or `high`. For more details on how images are processed and associated costs, refer to the [OpenAI API documentation](https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding).\n",
    "\n",
    "Next let's use some helper functions to create another prompt that we will use to send a request to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant.'},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'Based on these infographics, can you summarize how Semantic Kernel works in exactly one sentence?'},\n",
       "   {'type': 'image_url',\n",
       "    'image_url': {'url': 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAADKsA...',\n",
       "     'detail': 'high'}},\n",
       "   {'type': 'image_url',\n",
       "    'image_url': {'url': 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAADWAA...',\n",
       "     'detail': 'low'}}]}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from not_again_ai.llm.openai_api.prompts import chat_prompt\n",
    "\n",
    "sk_infographic = Path.cwd().parent.parent / \"tests\" / \"llm\" / \"sample_images\" / \"SKInfographic.png\"\n",
    "sk_diagram = Path.cwd().parent.parent / \"tests\" / \"llm\" / \"sample_images\" / \"SKDiagram.png\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful {{ persona }}.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            \"Based on these infographics, can you summarize how {{ library }} works in exactly one sentence?\",\n",
    "            {\"image\": sk_infographic, \"detail\": \"high\"},\n",
    "            {\"image\": sk_diagram, \"detail\": \"low\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = chat_prompt(messages, variables={\"persona\": \"assistant\", \"library\": \"Semantic Kernel\"})\n",
    "\n",
    "# Truncate the url fields to avoid cluttering the output\n",
    "prompt[1][\"content\"][1][\"image_url\"][\"url\"] = prompt[1][\"content\"][1][\"image_url\"][\"url\"][0:50] + \"...\"\n",
    "prompt[1][\"content\"][2][\"image_url\"][\"url\"] = prompt[1][\"content\"][2][\"image_url\"][\"url\"][0:50] + \"...\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the two images that were encoded:\n",
    "\n",
    "![SKInfographic](https://github.com/DaveCoDev/not-again-ai/blob/main/tests/llm/sample_images/SKInfographic.png?raw=true)\n",
    "\n",
    "![SKDiagram](https://github.com/DaveCoDev/not-again-ai/blob/main/tests/llm/sample_images/SKDiagram.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making an API Request\n",
    "\n",
    "With prompt formatted, making the request is easy. \n",
    "\n",
    "As of the gpt-4-turbo version 2024-04-09, the vision model supports the same parameters as a text-only request.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9iRqWn5D2VcqwdUA6y67oBfmi3Mjp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The Semantic Kernel orchestrates a series of AI-driven functions and plugins to process tasks from user input, invoke appropriate AI services, and return the summarized results to the application.', role='assistant', function_call=None, tool_calls=None))], created=1720380500, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_4008e3b719', usage=CompletionUsage(completion_tokens=34, prompt_tokens=1565, total_tokens=1599))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = chat_prompt(messages, variables={\"persona\": \"assistant\", \"library\": \"Semantic Kernel\"})\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-2024-05-13\",\n",
    "    messages=prompt,\n",
    "    max_tokens=200,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying the response format\n",
    "\n",
    "The response from the API is quite verbose. We can simplify it by extracting only what is needed, depending on the parameters we provided in our request. \n",
    "\n",
    "Using our helper functions, let's send a request which tries to use all the available parameters. Notice that we use `n=2` to get two completions in one request. However, due to the seed they should always be equivalent. NOTE: We have noticed that the `seed` parameter is hit or miss and does not generate the same completions in all scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'finish_reason': 'stop',\n",
       "   'message': 'Semantic Kernel processes prompts through a series of plugins and APIs to generate and return appropriate task results to applications.'},\n",
       "  {'finish_reason': 'stop',\n",
       "   'message': 'Semantic Kernel processes user prompts by leveraging a series of plugins and AI services to recall memory, create plans, access APIs, and execute functions, ultimately returning the desired results to the application.'}],\n",
       " 'completion_tokens': 58,\n",
       " 'prompt_tokens': 1565,\n",
       " 'system_fingerprint': 'fp_4008e3b719',\n",
       " 'response_duration': 6.4716}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from not_again_ai.llm.openai_api.chat_completion import chat_completion\n",
    "from not_again_ai.llm.openai_api.openai_client import openai_client\n",
    "\n",
    "client = openai_client()\n",
    "\n",
    "response = chat_completion(\n",
    "    messages=prompt, model=\"gpt-4o-2024-05-13\", client=client, max_tokens=200, temperature=0.5, seed=42, n=2\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finish_reason': 'stop',\n",
       " 'message': 'Semantic Kernel processes prompts through a series of plugins and APIs to generate and return appropriate task results to applications.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we choose the default `n=1`, the response would look more like this:\n",
    "response[\"choices\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
