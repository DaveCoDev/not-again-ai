{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI Chat Completions\n",
    "\n",
    "This notebook covers how to use the Chat Completions API and other features such as creating prompts and function calling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the OpenAI Client\n",
    "\n",
    "The OpenAI client object is used to get responses from the API. This will automatically read your API and org key from your environment variables.\n",
    "\n",
    "You can optionally pass in your API key and org key as arguments: `api_key` and `organization`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from not_again_ai.llm.chat_completion.providers.openai_api import openai_client\n",
    "\n",
    "client = openai_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chat Completion\n",
    "\n",
    "The `chat_completion` function is an easy way to get responses from OpenAI models.\n",
    "It requires the prompt to the model to be formatted in the chat completion format,\n",
    "see the [API reference](https://platform.openai.com/docs/api-reference/chat/create) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from not_again_ai.llm.chat_completion import chat_completion\n",
    "from not_again_ai.llm.chat_completion.types import ChatCompletionRequest, SystemMessage, UserMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    UserMessage(content=\"Hello!\"),\n",
    "]\n",
    "request = ChatCompletionRequest(\n",
    "    messages=messages,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    max_completion_tokens=100,\n",
    ")\n",
    "response = chat_completion(request, \"openai\", client)\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Prompts\n",
    "\n",
    "Injecting variables into prompts is a common task and we provide the `chat_prompt` which uses [Liquid templating](https://jg-rp.github.io/liquid/).\n",
    "\n",
    "In the `messages` argument, the \"content\" field can be a [Python Liquid](https://jg-rp.github.io/liquid/introduction/getting-started) template string to allow for more dynamic prompts which not only supports variable injection, but also conditional logic, loops, and comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='- You are a helpful assistant trying to extract places that occur in a given text.\\n- You must identify all the places in the text and return them in a list like this: [\"place1\", \"place2\", \"place3\"].', role=<Role.SYSTEM: 'system'>, name=None),\n",
       " UserMessage(content='Here is the text I want you to extract places from:\\nI went to Paris and Berlin.', role=<Role.USER: 'user'>, name=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from not_again_ai.llm.prompting.compile_prompt import compile_messages\n",
    "\n",
    "place_extraction_prompt = [\n",
    "    SystemMessage(\n",
    "        content=\"\"\"- You are a helpful assistant trying to extract places that occur in a given text.\n",
    "- You must identify all the places in the text and return them in a list like this: [\"place1\", \"place2\", \"place3\"].\"\"\"\n",
    "    ),\n",
    "    UserMessage(\n",
    "        content=\"\"\"Here is the text I want you to extract places from:\n",
    "{%- # The user's input text goes below %}\n",
    "{{text}}\"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "variables = {\n",
    "    \"text\": \"I went to Paris and Berlin.\",\n",
    "}\n",
    "\n",
    "messages = compile_messages(messages=place_extraction_prompt, variables=variables)\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Management\n",
    "\n",
    "While the OpenAI chat completion will return the tokens used, the `num_tokens_from_messages` helper can be used to compute the number of tokens used in a list of messages before calling the API.\n",
    "\n",
    "We explicitly require a tokenizer since loading it has some overhead, so we want to avoid doing so many times for certain use cases.\n",
    "\n",
    "NOTE: This function not support counting tokens used by function calling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    }
   ],
   "source": [
    "from not_again_ai.llm.prompting.providers.openai_tiktoken import TokenizerOpenAI\n",
    "\n",
    "tokenizer = TokenizerOpenAI(model=\"gpt-4o-mini-2024-07-18\")\n",
    "num_tokens = tokenizer.num_tokens_in_messages(messages=messages)\n",
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion with Function Calling and other Parameters\n",
    "\n",
    "The `chat_completion` function can also be used to call functions in the prompt and a myriad of other commonly used parameters like temperature, max_tokens, and logprobs. See the docstring for more details.\n",
    "\n",
    "See the [gpt-4-v.ipynb](gpt-4-v.ipynb) for full details on how to use the vision features of `chat_completion` and `chat_prompt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionResponse(choices=[ChatCompletionChoice(message=AssistantMessage(content='', role=<Role.ASSISTANT: 'assistant'>, name=None, refusal=None, tool_calls=[ToolCall(id='call_dwyBECUXUbPyuJH6oGxE3DFz', function=Function(name='get_current_weather', arguments={'location': 'Boston, MA', 'format': 'fahrenheit'}), type='function')]), finish_reason='tool_calls', json_message=None, logprobs=None, extras={}), ChatCompletionChoice(message=AssistantMessage(content='', role=<Role.ASSISTANT: 'assistant'>, name=None, refusal=None, tool_calls=[ToolCall(id='call_yB03eV0flHXXKI6STtHMvPpm', function=Function(name='get_current_weather', arguments={'location': 'Boston, MA', 'format': 'fahrenheit'}), type='function')]), finish_reason='tool_calls', json_message=None, logprobs=None, extras={})], errors='', completion_tokens=46, prompt_tokens=99, completion_detailed_tokens=None, prompt_detailed_tokens=None, response_duration=0.8277, system_fingerprint='fp_e4fa3702df', extras={'prompt_filter_results': None})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a tool to get the current weather\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"format\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\", \"format\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "# Ask the model to call the function\n",
    "messages = [\n",
    "    UserMessage(\n",
    "        content=\"What's the current weather like in {{ city_state }} today? Call the get_current_weather function.\",\n",
    "    )\n",
    "]\n",
    "\n",
    "messages = compile_messages(messages=messages, variables={\"city_state\": \"Boston, MA\"})\n",
    "\n",
    "client = openai_client()\n",
    "\n",
    "request = ChatCompletionRequest(\n",
    "    messages=messages,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    client=client,\n",
    "    tools=tools,\n",
    "    tool_choice=\"required\",  # Force the model to use the tool\n",
    "    max_completion_tokens=300,\n",
    "    temperature=0,\n",
    "    log_probs=True,\n",
    "    top_log_probs=2,  # returns the log probabilities of the top 2 tokens\n",
    "    seed=42,  # Set the seed for reproducibility. The API will also return a `system_fingerprint` field to monitor changes in the backend.\n",
    "    n=2,  # Generate 2 completions at once\n",
    ")\n",
    "response = chat_completion(request, \"openai\", client)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
