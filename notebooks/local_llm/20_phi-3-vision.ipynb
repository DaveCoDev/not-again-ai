{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-3-Vision\n",
    "On May 21, 2024 Microsoft released phi-3-vision which is a 4.2B parameter multimodal model with language and vision capabilities. \n",
    "A [cookbook](https://github.com/microsoft/Phi-3CookBook) is available alongside the model being openly available on [Hugging Face](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct).\n",
    "\n",
    "As of May, the model is not available on inference tools like llama.cpp or Ollama which would make it easier to use the model for inference. \n",
    "This notebook is an example for how to setup it up using Hugging Face's transformers library. This does mean that the setup will be quite involved and may not work on many systems.\n",
    "\n",
    "\n",
    "## Installation\n",
    "1. Prerequisites:\n",
    "    - Linux\n",
    "    - Python 3.11 or 3.12\n",
    "    - Modern NVIDIA GPU (3000 Series or higher) with >=16GB\n",
    "1. Install PyTorch - [Getting Started](https://pytorch.org/get-started/locally/).\n",
    "    - Usually this is: `pip install torch torchvision torchaudio`\n",
    "1. Install [flash-attn](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features)\n",
    "    - `pip install packaging`\n",
    "    - `pip install flash-attn --no-build-isolation` \n",
    "    - May need install these first:\n",
    "        ```bash\n",
    "        pip install setuptools.\n",
    "        pip install wheel\n",
    "        ```\n",
    "1. `pip install accelerate`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from not_again_ai.local_llm.huggingface.chat_completion import chat_completion_image\n",
    "from not_again_ai.local_llm.huggingface.helpers import load_model, load_processor\n",
    "\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85121714b44f4eb5a9c6b9c0775a6fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_id=model_id)\n",
    "processor = load_processor(model_id=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave1/git-repos/not-again-ai/.venv/lib/python3.11/site-packages/PIL/Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The image displays a flowchart representing the process flow of an AI service within the Semantic Kernel framework. It shows the steps from invoking a prompt to selecting an AI service, rendering the prompt, and finally, invoking the AI service. The process includes model selection, templateization, reliability checks, and event notifications.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Your goal is to understand and describe images.\"},\n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"},\n",
    "]\n",
    "\n",
    "sk_diagram = Path.cwd().parent.parent / \"tests\" / \"llm\" / \"sample_images\" / \"SKDiagram.png\"\n",
    "images = [sk_diagram]\n",
    "\n",
    "response = chat_completion_image(\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    model_processor=(model, processor),\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "response[\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A handwritten number set'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Your goal is to understand and describe images in just a few words.\"},\n",
    "    {\"role\": \"user\", \"content\": \"<|image_1|>\\nWhat is shown in this image?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A cat\"},\n",
    "    {\"role\": \"user\", \"content\": \"<|image_2|>\\nWhat is shown in this image?\"},\n",
    "]\n",
    "\n",
    "cat = Path.cwd().parent.parent / \"tests\" / \"llm\" / \"sample_images\" / \"cat.jpg\"\n",
    "numbers = Path.cwd().parent.parent / \"tests\" / \"llm\" / \"sample_images\" / \"numbers.png\"\n",
    "images = [cat, numbers]\n",
    "\n",
    "response = chat_completion_image(\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    model_processor=(model, processor),\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "response[\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion Tokens: 46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The first image shows a cat's face with green eyes, a pink nose, and white whiskers. The second image appears to be a handwritten set of numbers from 1 to 12.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Your goal is to understand and describe images in just a few words.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<|image_1|><|image_2|>\\nWhat is shown in these images? Describe the first image first. Then describe the second.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "cat = Path.cwd().parent.parent / \"tests\" / \"llm\" / \"sample_images\" / \"cat.jpg\"\n",
    "numbers = Path.cwd().parent.parent / \"tests\" / \"llm\" / \"sample_images\" / \"numbers.png\"\n",
    "images = [cat, numbers]\n",
    "\n",
    "response = chat_completion_image(\n",
    "    messages=messages,\n",
    "    images=images,\n",
    "    model_processor=(model, processor),\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "print(f\"Completion Tokens: {response['completion_tokens']}\")\n",
    "response[\"message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2+2 is 4.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try inference without images\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is 2+2?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "response = chat_completion_image(\n",
    "    messages=messages,\n",
    "    images=None,\n",
    "    model_processor=(model, processor),\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "response[\"message\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
