{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Ollama\n",
    "\n",
    "[Ollama](https://github.com/ollama/ollama) is a simple way to get started with running language models locally.\n",
    "\n",
    "We provide helpers for interfacing with Ollama through the [ollama-python](https://github.com/ollama/ollama-python) package.\n",
    "\n",
    "## Installation\n",
    "1. Follow the instructions to install ollama for your system: https://github.com/ollama/ollama\n",
    "1. [Add Ollama as a startup service (recommended)](https://github.com/ollama/ollama/blob/main/docs/linux.md#adding-ollama-as-a-startup-service-recommended)\n",
    "1. If you'd like to make the ollama service accessible on your local network and it is hosted on Linux, add the following to the `/etc/systemd/system/ollama.service` file:\n",
    "    ```bash\n",
    "    [Service]\n",
    "    ...\n",
    "    Environment=\"OLLAMA_HOST=0.0.0.0\"\n",
    "    ```\n",
    "    Now ollama will be available at `http://<local_address>:11434`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the Ollama client\n",
    "\n",
    "We use the `Client` class from Ollama to allow customizability of the host. By default, the `ollama_client` function will try to read in the `OLLAMA_HOST` environment variable. If it is not set, you must provide a host. Generally, the default is `http://localhost:11434`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from not_again_ai.local_llm.ollama.ollama_client import ollama_client\n",
    "\n",
    "client = ollama_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chat Completion\n",
    "\n",
    "The `chat_completion` function can be used to call models.\n",
    "\n",
    "We assume that the model `phi3` has already been pulled into Ollama. If not, you can do so with the command `ollama pull phi3` in your terminal. Alternatively, you can use the `not_again_ai.llm.ollama.service.pull(model_name)` function to do so (we show this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': \" Hello there! How can I assist you today? Whether it's answering questions, providing information or helping with any tasks, feel free to let me know what you need. I'm here to help!\",\n",
       " 'prompt_tokens': 6,\n",
       " 'completion_tokens': 43,\n",
       " 'response_duration': 3.2580406665802}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from not_again_ai.local_llm.ollama.chat_completion import chat_completion\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "]\n",
    "\n",
    "response = chat_completion(messages, model=\"phi3\", client=client)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion with Other Features\n",
    "\n",
    "The Ollama API also supports several other features, such as JSON mode, temperature, and max_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': {'random_number': 42},\n",
       " 'prompt_tokens': 25,\n",
       " 'completion_tokens': 10,\n",
       " 'response_duration': 3.268199920654297}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a random number between 0 and 100 and structure the response in using JSON.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "response = chat_completion(\n",
    "    messages,\n",
    "    model=\"phi3\",\n",
    "    client=client,\n",
    "    max_tokens=300,\n",
    "    context_window=1000,\n",
    "    temperature=1.51,\n",
    "    json_mode=True,\n",
    "    seed=6,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Service Management\n",
    "\n",
    "Ollama models can also be managed using several helper functions. For example, you can pull a model, list all models, and delete a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'phi3:latest', 'model': 'phi3:latest', 'modified_at': '2024-06-20T00:22:45.763407206Z', 'size': 2393232963, 'size_readable': '2.23 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'phi3', 'families': ['phi3'], 'parameter_size': '3.8B', 'quantization_level': 'Q4_K_M'}}, {'name': 'command-r:35b', 'model': 'command-r:35b', 'modified_at': '2024-06-19T14:39:39.871170656Z', 'size': 20229443783, 'size_readable': '18.84 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'command-r', 'families': ['command-r'], 'parameter_size': '35B', 'quantization_level': 'Q4_0'}}, {'name': 'mistral:7b-instruct', 'model': 'mistral:7b-instruct', 'modified_at': '2024-06-19T14:19:39.805655483Z', 'size': 4113301090, 'size_readable': '3.83 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '7.2B', 'quantization_level': 'Q4_0'}}, {'name': 'llama3-gradient:70b', 'model': 'llama3-gradient:70b', 'modified_at': '2024-06-19T14:14:30.985214711Z', 'size': 39969745347, 'size_readable': '37.22 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '71B', 'quantization_level': 'Q4_0'}}, {'name': 'gemma:7b-instruct', 'model': 'gemma:7b-instruct', 'modified_at': '2024-06-19T14:04:55.476937521Z', 'size': 5011853225, 'size_readable': '4.67 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'gemma', 'families': ['gemma'], 'parameter_size': '9B', 'quantization_level': 'Q4_0'}}, {'name': 'llama3-gradient:8b', 'model': 'llama3-gradient:8b', 'modified_at': '2024-06-19T14:03:41.187868798Z', 'size': 4661224642, 'size_readable': '4.34 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '8B', 'quantization_level': 'Q4_0'}}, {'name': 'granite-code:20b', 'model': 'granite-code:20b', 'modified_at': '2024-06-19T14:02:40.883001184Z', 'size': 11552491057, 'size_readable': '10.76 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'starcoder', 'families': ['starcoder'], 'parameter_size': '20.1B', 'quantization_level': 'Q4_0'}}, {'name': 'codestral:22b', 'model': 'codestral:22b', 'modified_at': '2024-06-19T14:00:01.724710982Z', 'size': 12569170041, 'size_readable': '11.71 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '22.2B', 'quantization_level': 'Q4_0'}}, {'name': 'codellama:13b', 'model': 'codellama:13b', 'modified_at': '2024-06-19T13:57:03.810150065Z', 'size': 7365960935, 'size_readable': '6.86 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': None, 'parameter_size': '13B', 'quantization_level': 'Q4_0'}}, {'name': 'qwen2:7b', 'model': 'qwen2:7b', 'modified_at': '2024-06-19T13:54:32.659973485Z', 'size': 4431400262, 'size_readable': '4.13 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '7.6B', 'quantization_level': 'Q4_0'}}, {'name': 'deepseek-coder-v2:16b', 'model': 'deepseek-coder-v2:16b', 'modified_at': '2024-06-19T13:49:37.607720887Z', 'size': 8905125528, 'size_readable': '8.29 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'deepseek2', 'families': ['deepseek2'], 'parameter_size': '15.7B', 'quantization_level': 'Q4_0'}}, {'name': 'phi3:medium', 'model': 'phi3:medium', 'modified_at': '2024-06-15T21:14:30.330581995Z', 'size': 7897126241, 'size_readable': '7.35 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'phi3', 'families': ['phi3'], 'parameter_size': '14.0B', 'quantization_level': 'F16'}}, {'name': 'llama3:8b', 'model': 'llama3:8b', 'modified_at': '2024-05-07T12:50:01.227242442Z', 'size': 4661224578, 'size_readable': '4.34 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '8B', 'quantization_level': 'Q4_0'}}]\n"
     ]
    }
   ],
   "source": [
    "from not_again_ai.local_llm.ollama.service import delete, is_model_available, list_models, pull, show\n",
    "\n",
    "# Check what models are installed\n",
    "models = list_models(client)\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a model is available\n",
    "is_model_available(\"phi3\", client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelfile': '# Modelfile generated by \"ollama show\"\\n# To build a new Modelfile based on this, replace FROM with:\\n# FROM phi3:latest\\n\\nFROM /usr/share/ollama/.ollama/models/blobs/sha256-b26e6713dc749dda35872713fa19a568040f475cc71cb132cff332fe7e216462\\nTEMPLATE \"{{ if .System }}<|system|>\\n{{ .System }}<|end|>\\n{{ end }}{{ if .Prompt }}<|user|>\\n{{ .Prompt }}<|end|>\\n{{ end }}<|assistant|>\\n{{ .Response }}<|end|>\"\\nPARAMETER stop <|end|>\\nPARAMETER stop <|user|>\\nPARAMETER stop <|assistant|>\\nLICENSE \"\"\"Microsoft.\\nCopyright (c) Microsoft Corporation.\\n\\nMIT License\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\"\"\"\\n',\n",
       " 'parameters': 'stop                           \"<|end|>\"\\nstop                           \"<|user|>\"\\nstop                           \"<|assistant|>\"',\n",
       " 'template': '{{ if .System }}<|system|>\\n{{ .System }}<|end|>\\n{{ end }}{{ if .Prompt }}<|user|>\\n{{ .Prompt }}<|end|>\\n{{ end }}<|assistant|>\\n{{ .Response }}<|end|>',\n",
       " 'details': {'parent_model': '',\n",
       "  'format': 'gguf',\n",
       "  'family': 'phi3',\n",
       "  'families': ['phi3'],\n",
       "  'parameter_size': '3.8B',\n",
       "  'quantization_level': 'Q4_K_M'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show details about a model\n",
    "show(\"phi3\", client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete a model\n",
    "delete(\"phi3\", client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull a model\n",
    "pull(\"phi3\", client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
