{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Ollama\n",
    "\n",
    "[Ollama](https://github.com/ollama/ollama) is a simple way to get started with running language models locally.\n",
    "\n",
    "We provide helpers to interface with Ollama by wrapping the [ollama-python](https://github.com/ollama/ollama-python) package.\n",
    "\n",
    "## Installation\n",
    "See the main README for installation instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the Ollama client\n",
    "\n",
    "We use the `Client` class from Ollama to allow customizability of the host. By default, the `ollama_client` function will try to read in the `OLLAMA_HOST` environment variable. If it is not set, you must provide a host. Generally, the default is `http://localhost:11434`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from not_again_ai.local_llm.ollama.ollama_client import ollama_client\n",
    "\n",
    "client = ollama_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chat Completion\n",
    "\n",
    "The `chat_completion` function can be used to call models.\n",
    "\n",
    "We assume that the model `phi3` has already been pulled into Ollama. If not, you can do so with the command `ollama pull phi3` in your terminal. Alternatively, you can use the `not_again_ai.llm.ollama.service.pull(model_name)` function to do so (we show this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': ' \"Hello!\"\\n\\n\\nThis simple greeting is appropriate for various contexts, including casual conversations and initial interactions with someone new where no prior relationship exists between the parties involved. It\\'s universally understood in many cultures as an informal way to acknowledge another person’ endlessly pursuing a solution without any substantial progress or feedback could be demotivating and counterproductive, potentially leading them down unhelpful paths due to lack of direction. To assist you effectively:\\n\\n- Offer constructive criticism if appropriate – give specific suggestions on how they might improve their approach in seeking solutions for the common cold while emphasizing empathy.',\n",
       " 'prompt_tokens': 13,\n",
       " 'completion_tokens': 137,\n",
       " 'response_duration': 0.892}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from not_again_ai.local_llm.ollama.chat_completion import chat_completion\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "]\n",
    "\n",
    "response = chat_completion(messages, model=\"phi3\", client=client)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completion with Other Features\n",
    "\n",
    "The Ollama API also supports several other features, such as JSON mode, temperature, and max_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': {'random_number': '42'},\n",
       " 'prompt_tokens': 32,\n",
       " 'completion_tokens': 13,\n",
       " 'response_duration': 2.7096}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a random number between 0 and 100 and structure the response in using JSON.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "response = chat_completion(\n",
    "    messages,\n",
    "    model=\"phi3\",\n",
    "    client=client,\n",
    "    max_tokens=300,\n",
    "    context_window=1000,\n",
    "    temperature=1.51,\n",
    "    json_mode=True,\n",
    "    seed=6,\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Service Management\n",
    "\n",
    "Ollama models can also be managed using several helper functions. For example, you can pull a model, list all models, and delete a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'llama3.1:70b-instruct-q4_0', 'model': 'llama3.1:70b-instruct-q4_0', 'modified_at': '2024-07-27T19:48:08.427121819Z', 'size': 39969747075, 'size_readable': '37.22 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '70.6B', 'quantization_level': 'Q4_0'}}, {'name': 'llama3.1:8b-instruct-q4_0', 'model': 'llama3.1:8b-instruct-q4_0', 'modified_at': '2024-07-27T19:41:30.293348579Z', 'size': 4661226402, 'size_readable': '4.34 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '8.0B', 'quantization_level': 'Q4_0'}}, {'name': 'phi3:3.8b-mini-4k-instruct-q8_0', 'model': 'phi3:3.8b-mini-4k-instruct-q8_0', 'modified_at': '2024-07-27T19:25:48.923425749Z', 'size': 4061223105, 'size_readable': '3.78 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'phi3', 'families': ['phi3'], 'parameter_size': '3.8B', 'quantization_level': 'Q8_0'}}, {'name': 'phi3:latest', 'model': 'phi3:latest', 'modified_at': '2024-07-07T19:32:58.041493035Z', 'size': 2176178401, 'size_readable': '2.03 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'phi3', 'families': ['phi3'], 'parameter_size': '3.8B', 'quantization_level': 'Q4_0'}}, {'name': 'gemma2:27b', 'model': 'gemma2:27b', 'modified_at': '2024-06-29T16:04:08.903884093Z', 'size': 15628387569, 'size_readable': '14.56 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'gemma2', 'families': ['gemma2'], 'parameter_size': '27.2B', 'quantization_level': 'Q4_0'}}, {'name': 'deepseek-coder-v2:16b', 'model': 'deepseek-coder-v2:16b', 'modified_at': '2024-06-23T11:34:07.289052739Z', 'size': 8905125527, 'size_readable': '8.29 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'deepseek2', 'families': ['deepseek2'], 'parameter_size': '15.7B', 'quantization_level': 'Q4_0'}}, {'name': 'llama3:70b', 'model': 'llama3:70b', 'modified_at': '2024-06-22T19:28:46.130473755Z', 'size': 39969745349, 'size_readable': '37.22 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '70.6B', 'quantization_level': 'Q4_0'}}, {'name': 'command-r:35b', 'model': 'command-r:35b', 'modified_at': '2024-06-19T14:39:39.871170656Z', 'size': 20229443783, 'size_readable': '18.84 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'command-r', 'families': ['command-r'], 'parameter_size': '35B', 'quantization_level': 'Q4_0'}}, {'name': 'mistral:7b-instruct', 'model': 'mistral:7b-instruct', 'modified_at': '2024-06-19T14:19:39.805655483Z', 'size': 4113301090, 'size_readable': '3.83 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '7.2B', 'quantization_level': 'Q4_0'}}, {'name': 'llama3-gradient:70b', 'model': 'llama3-gradient:70b', 'modified_at': '2024-06-19T14:14:30.985214711Z', 'size': 39969745347, 'size_readable': '37.22 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '71B', 'quantization_level': 'Q4_0'}}, {'name': 'gemma:7b-instruct', 'model': 'gemma:7b-instruct', 'modified_at': '2024-06-19T14:04:55.476937521Z', 'size': 5011853225, 'size_readable': '4.67 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'gemma', 'families': ['gemma'], 'parameter_size': '9B', 'quantization_level': 'Q4_0'}}, {'name': 'llama3-gradient:8b', 'model': 'llama3-gradient:8b', 'modified_at': '2024-06-19T14:03:41.187868798Z', 'size': 4661224642, 'size_readable': '4.34 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '8B', 'quantization_level': 'Q4_0'}}, {'name': 'granite-code:20b', 'model': 'granite-code:20b', 'modified_at': '2024-06-19T14:02:40.883001184Z', 'size': 11552491057, 'size_readable': '10.76 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'starcoder', 'families': ['starcoder'], 'parameter_size': '20.1B', 'quantization_level': 'Q4_0'}}, {'name': 'codestral:22b', 'model': 'codestral:22b', 'modified_at': '2024-06-19T14:00:01.724710982Z', 'size': 12569170041, 'size_readable': '11.71 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '22.2B', 'quantization_level': 'Q4_0'}}, {'name': 'codellama:13b', 'model': 'codellama:13b', 'modified_at': '2024-06-19T13:57:03.810150065Z', 'size': 7365960935, 'size_readable': '6.86 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': None, 'parameter_size': '13B', 'quantization_level': 'Q4_0'}}, {'name': 'qwen2:7b', 'model': 'qwen2:7b', 'modified_at': '2024-06-19T13:54:32.659973485Z', 'size': 4431400262, 'size_readable': '4.13 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'qwen2', 'families': ['qwen2'], 'parameter_size': '7.6B', 'quantization_level': 'Q4_0'}}, {'name': 'phi3:medium', 'model': 'phi3:medium', 'modified_at': '2024-06-15T21:14:30.330581995Z', 'size': 7897126241, 'size_readable': '7.35 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'phi3', 'families': ['phi3'], 'parameter_size': '14.0B', 'quantization_level': 'F16'}}, {'name': 'llama3:8b', 'model': 'llama3:8b', 'modified_at': '2024-05-07T12:50:01.227242442Z', 'size': 4661224578, 'size_readable': '4.34 GB', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '8B', 'quantization_level': 'Q4_0'}}]\n"
     ]
    }
   ],
   "source": [
    "from not_again_ai.local_llm.ollama.service import delete, is_model_available, list_models, pull, show\n",
    "\n",
    "# Check what models are installed\n",
    "models = list_models(client)\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a model is available\n",
    "is_model_available(\"phi3\", client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelfile': '# Modelfile generated by \"ollama show\"\\n# To build a new Modelfile based on this, replace FROM with:\\n# FROM phi3:latest\\n\\nFROM /usr/share/ollama/.ollama/models/blobs/sha256-3e38718d00bb0007ab7c0cb4a038e7718c07b54f486a7810efd03bb4e894592a\\nTEMPLATE \"{{ if .System }}<|system|>\\n{{ .System }}<|end|>\\n{{ end }}{{ if .Prompt }}<|user|>\\n{{ .Prompt }}<|end|>\\n{{ end }}<|assistant|>\\n{{ .Response }}<|end|>\"\\nPARAMETER stop <|end|>\\nPARAMETER stop <|user|>\\nPARAMETER stop <|assistant|>\\nLICENSE \"\"\"Microsoft.\\nCopyright (c) Microsoft Corporation.\\n\\nMIT License\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\"\"\"\\n',\n",
       " 'parameters': 'stop                           \"<|end|>\"\\nstop                           \"<|user|>\"\\nstop                           \"<|assistant|>\"',\n",
       " 'template': '{{ if .System }}<|system|>\\n{{ .System }}<|end|>\\n{{ end }}{{ if .Prompt }}<|user|>\\n{{ .Prompt }}<|end|>\\n{{ end }}<|assistant|>\\n{{ .Response }}<|end|>',\n",
       " 'details': {'parent_model': '',\n",
       "  'format': 'gguf',\n",
       "  'family': 'phi3',\n",
       "  'families': ['phi3'],\n",
       "  'parameter_size': '3.8B',\n",
       "  'quantization_level': 'Q4_0'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show details about a model\n",
    "show(\"phi3\", client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete a model\n",
    "delete(\"phi3\", client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull a model\n",
    "pull(\"phi3\", client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
